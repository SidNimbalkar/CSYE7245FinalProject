# -*- coding: utf-8 -*-
"""Twitter-Scraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w5S4t8QF9OvgEvSGf21F2Q7MIhHEwZst
"""

import tweepy as tw
consumer_key = "w9Dj65V9mr4v1OCml72rlersQ"
consumer_secret = "PRpUhWbRvGQ1o3xkdalGUGlB1cwjNSmjFshba09e0nKdafcuHY"
access_token = "838352454517424128-aUaIuR6IgUyUZzwdOZ3JvVPlaDoJrYw"
access_token_secret = "ENAPFU3NUDYHB79kdGwjPGlDYQFhTUh0BIYElcCX2DTI5"
auth = tw.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tw.API(auth,wait_on_rate_limit=True)

"""# Scraping"""

import pandas as pd
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
tweets = []
public_tweets = api.home_timeline(count = 300)
for tweet in public_tweets:
  tweets.append(tweet.text)

list = pd.DataFrame({'tweets' : tweets})
list

from collections import Counter
common_words = Counter(" ".join(list["tweets"]).split()).most_common(100)

first_tuple_elements = []

for a_tuple in common_words:
#Sequentially access each tuple in `tuple_list`

	first_tuple_elements.append(a_tuple[0])
#print(first_tuple_elements)
search_words = [word for word in first_tuple_elements if word not in stopwords.words('english')]
search_words

for word in search_words:
  tweets = tw.Cursor(api.search,
              q=word,
              lang="en",
              since='2018-04-23').items(25000)
scraped_tweets = [tweet.text for tweet in tweets]
scraped_tweets

"""# Creating a dataframe"""

all_tweets = pd.DataFrame({'tweets' : scraped_tweets})

all_tweets

all_tweets.to_csv('dataset.csv')
all_tweets.to_excel('scraped_tweets.xlsx')
"""# Preprocessing"""

import re
import pandas as pd
from nltk import tokenize
import string
import nltk
from nltk.corpus import stopwords
from smart_open import smart_open

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('words')
nltk.download('wordnet')
 


def remove_punct(text):
    text  = "".join([char for char in text if char not in string.punctuation])
    text = re.sub('[0-9]+', '', text)
    return text
all_tweets['Tweet_punct'] = all_tweets['tweets'].apply(lambda x: remove_punct(x))

def tokenization(text):
  text = re.split('\W+', text)
  return text
all_tweets['Tweet_tokenized'] = all_tweets['Tweet_punct'].apply(lambda x: tokenization(x.lower()))

stopword = nltk.corpus.stopwords.words('english')
def remove_stopwords(text):
    text = [word for word in text if word not in stopword]
    return text
all_tweets['Tweet_nonstop'] = all_tweets['Tweet_tokenized'].apply(lambda x: remove_stopwords(x))

ps = nltk.PorterStemmer()

def stemming(text):
    text = [ps.stem(word) for word in text]
    return text
all_tweets['Tweet_stemmed'] = all_tweets['Tweet_nonstop'].apply(lambda x: stemming(x))
        
wn = nltk.WordNetLemmatizer()

def lemmatizer(text):
    str = " "
    text = [wn.lemmatize(word) for word in text]
    return str.join(text)
all_tweets['Tweet_lemmatized'] = all_tweets['Tweet_nonstop'].apply(lambda x: lemmatizer(x))
  

all_tweets

"""# Labelling using Amazon Comprehend API"""

import json
key = 'AKIAJG5BRIEPKOSCS5WQ' 
sec_key = 'LmFTVSfFhQgJTuL/hHlKHw2XGpYXI5ShjvqCi7t6'


comprehend = boto3.client(service_name = 'comprehend', aws_access_key_id=key, aws_secret_access_key=sec_key, region_name='us-east-1')
                
clean_tweet = all_tweets['tweets'].head(10)

print('Calling DetectSentiment')
for tweet in clean_tweet:
  print(json.dumps(comprehend.detect_sentiment(Text=tweet, LanguageCode='en'), sort_keys=True, indent=4))
print('End of DetectSentiment\n')

